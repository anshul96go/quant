{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "\n",
    "1. Reference Python Code for Selenium + BeautifulSoup while working with API\n",
    "\n",
    "\n",
    "2. Selenium for extracting data from website\n",
    "\n",
    "\n",
    "3. Scraping financial website: Estimize\n",
    "\n",
    "\n",
    "4. Attempt to scrap LinkedIn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "Requirement already satisfied: urllib3 in /home/anshul/anaconda3/lib/python3.7/site-packages (from selenium) (1.25.8)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reference Python Code for Selenium + BeautifulSoup while working with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import shlex\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the page\n",
    "#driver = webdriver.Chrome(\"E:/Data Science/chromedriver_win32 (2)/chromedriver.exe\")\n",
    "driver = webdriver.Firefox(executable_path=r'C:/Users/anshu/Downloads/geckodriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert to readable time\n",
    "def get_time(time):\n",
    "    ts = int(time)\n",
    "    # if you encounter a \"year is out of range\" error the timestamp\n",
    "    # may be in milliseconds, try `ts /= 1000` in that case\n",
    "    return (datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def random_wait(tm=5):\n",
    "    t=(1+random.randint(0, tm-1))\n",
    "    time.sleep(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Declare initial parameters\n",
    "'''\n",
    "end_time_int = 1684641599\n",
    "start_time_int = 1367121600\n",
    "duration = 604799\n",
    "end = end_time_int\n",
    "res_df = pd.DataFrame()\n",
    "itr=1\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Open the browser\n",
    "'''\n",
    "driver = webdriver.Firefox(executable_path=r'C:/Users/anshu/Downloads/geckodriver.exe')\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Main Code\n",
    "'''\n",
    "while end>=start_time_int:\n",
    "    print(\"itr: \",itr)\n",
    "    '''\n",
    "    Get the start date of each period\n",
    "    '''\n",
    "    start = end - duration\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Extract Data for the designated duration\n",
    "    '''\n",
    "    ## if no data is obtained, break the loop ##\n",
    "    # api link\n",
    "    link = \"https://api.coinmarketcap.com/data-api/v3/cryptocurrency/detail/chart?id=1&range={}~{}\".format(start,end)\n",
    "    \n",
    "    # open webpage\n",
    "    driver.get(link)\n",
    "    random_wait(3)\n",
    "\n",
    "    # click to expand\n",
    "    driver.find_element_by_css_selector(\"button[class='btn expand']\").click()\n",
    "    random_wait(2)\n",
    "    \n",
    "    # get the soup\n",
    "    soup = bs.BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    # get the time rows\n",
    "    timestamp_list = soup.find_all(\"tr\",{'aria-level':'3'})\n",
    "    \n",
    "    # if no data is obtained, break the loop\n",
    "    if len(timestamp_list)==0:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Convert to DataFrame\n",
    "    '''\n",
    "    res_list = []\n",
    "    for timestamp in timestamp_list:\n",
    "\n",
    "        # try to extract without beautiful soup\n",
    "        time_str = timestamp.text\n",
    "        link_0 = \"/data/points/{}/c/0\".format(time_str)\n",
    "        link_1 = \"/data/points/{}/c/1\".format(time_str)\n",
    "        link_2 = \"/data/points/{}/c/2\".format(time_str)\n",
    "        price = driver.find_element_by_css_selector(\"tr[id='{}']\".format(link_0)).text[2:]\n",
    "        vol = driver.find_element_by_css_selector(\"tr[id='{}']\".format(link_1)).text[2:]\n",
    "        mcap = driver.find_element_by_css_selector(\"tr[id='{}']\".format(link_2)).text[2:]\n",
    "        \n",
    "        # store it in dataframe\n",
    "        res_list.append({'time':time_str,\n",
    "                         'price':price,\n",
    "                         'vol_past24hr':vol,\n",
    "                         'mcap':mcap})\n",
    "\n",
    "\n",
    "    tmp_df = pd.DataFrame(res_list)\n",
    "    tmp_df['time_new'] = tmp_df.apply(lambda x: get_time(x['time']),axis=1)    \n",
    "    res_df = pd.concat([res_df,tmp_df])\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Update the end date for next duration\n",
    "    '''\n",
    "    end = start-1\n",
    "    itr+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inefficient with beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Declare initial parameters\n",
    "# '''\n",
    "# end_time_int = 1684641599\n",
    "# start_time_int = 1678593609  # 1367121600\n",
    "# duration = 604799\n",
    "# end = end_time_int\n",
    "# res_df = pd.DataFrame()\n",
    "# itr=1\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# Open the browser\n",
    "# '''\n",
    "# driver = webdriver.Firefox(executable_path=r'C:/Users/anshu/Downloads/geckodriver.exe')\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# Main Code\n",
    "# '''\n",
    "# while end>=start_time_int:\n",
    "#     print(\"itr: \",itr)\n",
    "#     '''\n",
    "#     Get the start date of each period\n",
    "#     '''\n",
    "#     start = end - duration\n",
    "    \n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     Extract Data for the designated duration\n",
    "#     '''\n",
    "#     ## if no data is obtained, break the loop ##\n",
    "#     # api link\n",
    "#     link = \"https://api.coinmarketcap.com/data-api/v3/cryptocurrency/detail/chart?id=1&range={}~{}\".format(start,end)\n",
    "    \n",
    "#     # open webpage\n",
    "#     driver.get(link)\n",
    "#     random_wait(3)\n",
    "\n",
    "#     # click to expand\n",
    "#     driver.find_element_by_css_selector(\"button[class='btn expand']\").click()\n",
    "#     random_wait(2)\n",
    "    \n",
    "#     # get the soup\n",
    "#     soup = bs.BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "#     # get the time rows\n",
    "#     timestamp_list = soup.find_all(\"tr\",{'aria-level':'3'})\n",
    "    \n",
    "#     # if no data is obtained, break the loop\n",
    "#     if len(timestamp_list)==0:\n",
    "#         break\n",
    "    \n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     Convert to DataFrame\n",
    "#     '''\n",
    "#     res_list = []\n",
    "#     for timestamp in timestamp_list:\n",
    "\n",
    "#         # get timestamp\n",
    "#         time_str = timestamp.text\n",
    "\n",
    "#         # get c elements\n",
    "#         c_list = soup.find_all(\"tr\",id = re.compile(\"/data/points/{}/c/\".format(time_str)))\n",
    "\n",
    "#         # store it in dataframe\n",
    "#         res_list.append({'time':time_str,\n",
    "#                          'price':c_list[0].find(\"span\",class_='objectBox objectBox-number').text,\n",
    "#                          'vol_past24hr':c_list[1].find(\"span\",class_='objectBox objectBox-number').text,\n",
    "#                          'mcap':c_list[2].find(\"span\",class_='objectBox objectBox-number').text})\n",
    "\n",
    "\n",
    "#     tmp_df = pd.DataFrame(res_list)\n",
    "#     tmp_df['time_new'] = tmp_df.apply(lambda x: get_time(x['time']),axis=1)    \n",
    "#     res_df = pd.concat([res_df,tmp_df])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     Update the end date for next duration\n",
    "#     '''\n",
    "#     end = start-1\n",
    "#     itr+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Selenium for extracting data from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the website page\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.basspro.com/shop/en/guns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get guns category\n",
    "# gtype = driver.find_element_by_id(\"categoryFacetList_5_-2009_3074457345618347091\")\n",
    "# time.sleep(10)\n",
    "# options = gtype.find_elements_by_tag_name(\"li\")\n",
    "options = ['https://www.basspro.com/shop/en/centerfire-pistol','https://www.basspro.com/shop/en/shotgun',\n",
    "          'https://www.basspro.com/shop/en/centerfire-rifles','https://www.basspro.com/shop/en/rimfire-rifle',\n",
    "          'https://www.basspro.com/shop/en/rimfire-pistols']\n",
    "data = []\n",
    "for option in options:\n",
    "#     try:\n",
    "#         option.get_attribute(\"href\").click()\n",
    "#         print('click1')\n",
    "#         time.sleep(5)\n",
    "#     except:\n",
    "#         try: \n",
    "#             driver.get(option.get_attribute(\"href\"))\n",
    "#             print('click2')\n",
    "#             time.sleep(5)\n",
    "        \n",
    "#         except:\n",
    "#             time.sleep(5)\n",
    "    driver.get(option)\n",
    "    print('clicked!')\n",
    "    time.sleep(5)    \n",
    "#     page = driver.find_element_by_id(\"dijit__WidgetBase_0\")\n",
    "    i=0\n",
    "    it=1\n",
    "    while(i<2):\n",
    "        page = driver.find_element_by_css_selector(\"div [class='product_listing_container']\")\n",
    "        guns = page.find_elements_by_tag_name(\"li\")\n",
    "        print(\"PAGE NO. \",it)\n",
    "        try:\n",
    "            if not guns:\n",
    "                i=1000\n",
    "        except:\n",
    "            print(\"DATA LEFT\")\n",
    "            continue\n",
    "        for gun in guns:\n",
    "            name = gun.find_element_by_css_selector(\"div[class='product_name']\").text\n",
    "            price = gun.find_element_by_css_selector(\"span[itemprop='price']\").get_attribute(\"content\")\n",
    "            print(name,price)\n",
    "            data.append({'name':name,'price':price})\n",
    "        next_pg = driver.find_element_by_css_selector(\"i[alt='Show next page'][class='icon icon-arrow-right']\")\n",
    "        next_pg.click()\n",
    "        time.sleep(5)\n",
    "        it=it+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "keys = data[0].keys()\n",
    "with open('guns.csv', 'w', newline='')  as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <i [alt='Show next page'][class='icon icon-arrow-right'] \n",
    "# id=\"WC_SearchBasedNavigationResults_pagination_link_right_categoryResults_top_img\"></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <i alt=\"Show next page\" class=\"icon icon-arrow-right\" \n",
    "# id=\"WC_SearchBasedNavigationResults_pagination_link_right_categoryResults_top_img\"></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scraping financial website: Estimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time\n",
    "import os\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from lxml.html import fromstring\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "import re\n",
    "import random\n",
    "import pyautogui as py\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# from get_data_each_stock import *\n",
    "\n",
    "\n",
    "# 2. Open browser\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "\n",
    "    ### create config from proxy\n",
    "def create_proxy_config(proxy):\n",
    "    myProxy = proxy\n",
    "    proxy = Proxy({\n",
    "        'proxyType': ProxyType.MANUAL,\n",
    "        'httpProxy': myProxy,\n",
    "        'ftpProxy': myProxy,\n",
    "        'sslProxy': myProxy,\n",
    "        'noProxy': '' # set this value as desired\n",
    "        })\n",
    "    return proxy\n",
    "\n",
    "proxies = get_proxies()\n",
    "proxies = get_proxies()\n",
    "if len(proxies) ==0:\n",
    "    proxy = None\n",
    "    proxy_pool = None\n",
    "else:\n",
    "    proxy_pool = cycle(proxies)\n",
    "    proxy = next(proxy_pool)\n",
    "    proxy = create_proxy_config(proxy)\n",
    "\n",
    "def open_browser(proxy,proxy_pool):\n",
    "    try:\n",
    "        driver = webdriver.Firefox(executable_path=r'C:/Users/anshu/Downloads/geckodriver.exe',proxy=proxy)\n",
    "    except:\n",
    "        if len(proxy_pool)==0:\n",
    "            proxy=None\n",
    "        else:\n",
    "            proxy = next(proxy_pool)\n",
    "            proxy = create_proxy_config(proxy)\n",
    "            driver = webdriver.Firefox(executable_path=r'C:/Users/anshu/Downloads/geckodriver.exe',proxy=proxy)\n",
    "    time.sleep(2)\n",
    "    return driver,proxy,proxy_pool\n",
    "\n",
    "\n",
    "\n",
    "# initialise stock list\n",
    "stock_list = []\n",
    "\n",
    "# get drivers\n",
    "driver, proxy,proxy_pool = open_browser(proxy,proxy_pool)\n",
    "# driver.get('https://www.estimize.com/sectors/consumer-discretionary')\n",
    "\n",
    "# # click to get all stocks list\n",
    "# driver.find_element_by_css_selector('div[class=\"pagination-footer').click()\n",
    "# time.sleep(2)\n",
    "\n",
    "# # append list of links\n",
    "# elm_list = []\n",
    "# for elm in driver.find_elements_by_css_selector('a[class=\"linked-row closed'):\n",
    "#     elm = elm.get_attribute('href')\n",
    "#     elm = (elm[:-9])\n",
    "#     elm_list.append(elm)\n",
    "\n",
    "# if len(elm_list)==0:\n",
    "#     print(\"using opened one now\")\n",
    "#     for elm in driver.find_elements_by_css_selector('a[class=\"linked-row opened'):\n",
    "#         elm = elm.get_attribute('href')\n",
    "#         elm = (elm[:-9])\n",
    "#         elm_list.append(elm) \n",
    "\n",
    "# # click href link    \n",
    "# for elm in elm_list:    \n",
    "#     print(elm)\n",
    "#     try:\n",
    "#         stock_list = get_data_from_webpage_2(driver,elm,stock_list)\n",
    "#     except:\n",
    "#         continue\n",
    "\n",
    "# # save the stock_list as df\n",
    "# tmp_df = pd.DataFrame(stock_list)\n",
    "# tmp_df.to_csv('../output/stock_list_2.csv') \n",
    "\n",
    "\n",
    "def get_data_from_webpage_2(driver,elm,stock_list):    \n",
    "    \n",
    "    # get the webpage\n",
    "    driver.get(elm)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #click to get table\n",
    "    try:\n",
    "        driver.find_element_by_css_selector('a[class=\"estimate-bar__button\"]').click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"okay\")\n",
    "    \n",
    "    # get the name of result sheet\n",
    "    name = driver.find_element_by_css_selector('h1[class=\"release-header-information-title\"]').text\n",
    "    name = name.replace(\" \",\"\")\n",
    "    \n",
    "    # get the next earning dates\n",
    "    date_elm = driver.find_element_by_css_selector('span[class=\"multiform__countdown-reports-at\"]').text\n",
    "    day = date_elm.split()[1]\n",
    "    date = date_elm.split()[2]\n",
    "    timings = date_elm.split()[3]\n",
    "    print(day,date,timings)\n",
    "    \n",
    "    # save the name of company and link in the stock_list\n",
    "    stock_list.append({'name':name, 'link':elm, 'earn_day':day, 'earn_date':date, 'earn_time':timings})\n",
    "    \n",
    "    # create dataframe\n",
    "    res_df = pd.DataFrame()\n",
    "    \n",
    "    # get column names from top row\n",
    "    row = driver.find_element_by_css_selector('tr[class=\"multiform-tr-qtrs\"]')\n",
    "    cols = row.find_elements_by_css_selector('td[class=\"multiform-td-qtr\"]')\n",
    "    col_names = []\n",
    "    for col in cols:\n",
    "        col_names.append(col.text)\n",
    "    \n",
    "    # get estimize value and count columns\n",
    "    row = driver.find_element_by_css_selector('tr[class=\"multiform-tr-consensus multiform-tr-estimize\"]')\n",
    "    cols = row.find_elements_by_css_selector('td[class=\"multiform-td-data\"]')\n",
    "    est_val_dict = {}\n",
    "    est_cnt_dict = {}\n",
    "    i=0\n",
    "    for col in cols:\n",
    "        tmp = col.text.split()\n",
    "        try:\n",
    "            est_val_dict[col_names[i]] = tmp[0]\n",
    "        except:\n",
    "            est_val_dict[col_names[i]] = np.nan\n",
    "        try:\n",
    "            est_cnt_dict[col_names[i]] = tmp[1]\n",
    "        except:\n",
    "            est_cnt_dict[col_names[i]] = np.nan\n",
    "        i=i+1\n",
    "    est_val_df = pd.DataFrame([est_val_dict])\n",
    "    est_cnt_df = pd.DataFrame([est_cnt_dict])\n",
    "    res_df = pd.concat([res_df,est_val_df],ignore_index=True)\n",
    "    res_df = pd.concat([res_df,est_cnt_df],ignore_index=True)\n",
    "    \n",
    "    # get wall street columns\n",
    "    row = driver.find_element_by_css_selector('tr[class=\"multiform-tr-consensus multiform-tr-wallstreet\"]')\n",
    "    cols = row.find_elements_by_css_selector('td[class=\"multiform-td-data\"]')\n",
    "    wall_st_dict = {}\n",
    "    i=0\n",
    "    for col in cols:\n",
    "        wall_st_dict[col_names[i]] = col.text\n",
    "        i=i+1\n",
    "    wall_st_df = pd.DataFrame([wall_st_dict])\n",
    "    res_df = pd.concat([res_df,wall_st_df],ignore_index=True)\n",
    "    \n",
    "    # print(\"res: \",res_df)\n",
    "    \n",
    "    # get actual values columns\n",
    "    row = driver.find_element_by_css_selector('tr[class=\"multiform-tr-actuals\"]')\n",
    "    cols = row.find_elements_by_css_selector('td[class=\"multiform-td-data\"]')\n",
    "    act_val_dict = {}\n",
    "    act_gth_dict = {}\n",
    "    i=0\n",
    "    for col in cols:\n",
    "        tmp = col.text.split()\n",
    "        act_val_dict[col_names[i]] = tmp[0]\n",
    "        act_gth_dict[col_names[i]] = tmp[1]\n",
    "        i=i+1\n",
    "    act_val_df = pd.DataFrame([act_val_dict])\n",
    "    res_df = pd.concat([res_df, act_val_df],ignore_index=True)\n",
    "    act_gth_df = pd.DataFrame([act_gth_dict])\n",
    "    res_df = pd.concat([res_df, act_gth_df],ignore_index=True)\n",
    "    \n",
    "    # quit driver\n",
    "    # driver.close()\n",
    "    # print(\"res: \",res_df)\n",
    "    \n",
    "    # save dataframe\n",
    "    res_df.to_csv('../output/' + name + '.csv')\n",
    "    \n",
    "    return stock_list\n",
    "\n",
    "\n",
    "def get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = 'https://www.estimize.com/sectors/consumer-discretionary',op_file='stock_list_2.csv'):\n",
    "    # initiate stock_list\n",
    "    stock_list = []\n",
    "    \n",
    "    # open Consumer Durable webpage  \n",
    "    driver, proxy,proxy_pool = open_browser(proxy,proxy_pool)\n",
    "    driver.get(ind_link)\n",
    "    \n",
    "    # click to get all stocks list\n",
    "    driver.find_element_by_css_selector('div[class=\"pagination-footer').click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # append list of links\n",
    "    elm_list = []\n",
    "    for elm in driver.find_elements_by_css_selector('a[class=\"linked-row closed]'):\n",
    "        elm = elm.get_attribute('href')\n",
    "        elm = (elm[:-9])\n",
    "        elm_list.append(elm)\n",
    "        \n",
    "    # for elm in driver.find_elements_by_css_selector('a[class=\"linked-row opened]'):\n",
    "    #     elm = elm.get_attribute('href')\n",
    "    #     elm = (elm[:-9])\n",
    "    #     elm_list.append(elm)        \n",
    "    \n",
    "    if len(elm_list)==0:\n",
    "        print(\"using opened one now\")\n",
    "        for elm in driver.find_elements_by_css_selector('a[class=\"linked-row opened'):\n",
    "            elm = elm.get_attribute('href')\n",
    "            elm = (elm[:-9])\n",
    "            elm_list.append(elm) \n",
    "    \n",
    "    # click href link    \n",
    "    for elm in elm_list:    \n",
    "        print(elm)\n",
    "        try:\n",
    "            stock_list = get_data_from_webpage_2(driver,elm,stock_list)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # save the stock_list as df\n",
    "    try:\n",
    "        res_df = pd.read_csv('../output/'+op_file)\n",
    "    except:\n",
    "        res_df = pd.DataFrame()\n",
    "    tmp_df = pd.DataFrame(stock_list)\n",
    "    res_df = pd.concat([res_df,tmp_df],)\n",
    "    res_df.to_csv('../output/' + op_file)\n",
    "    return    \n",
    "\n",
    "# consumer discretionary\n",
    "lnk = 'https://www.estimize.com/sectors/consumer-discretionary'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')\n",
    "\n",
    "# consumer staple\n",
    "lnk = 'https://www.estimize.com/sectors/consumer-staples'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')\n",
    "\n",
    "# energy\n",
    "lnk = 'https://www.estimize.com/sectors/energy'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')\n",
    "\n",
    "# financials\n",
    "lnk = 'https://www.estimize.com/sectors/financials'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')\n",
    "\n",
    "# healthcare\n",
    "lnk = 'https://www.estimize.com/sectors/health-care'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')\n",
    "\n",
    "# industrials\n",
    "lnk = 'https://www.estimize.com/sectors/industrials'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')\n",
    "\n",
    "# it\n",
    "lnk = 'https://www.estimize.com/sectors/information-technology'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')\n",
    "\n",
    "# materials\n",
    "lnk = 'https://www.estimize.com/sectors/materials'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')\n",
    "\n",
    "# telecom\n",
    "lnk = 'https://www.estimize.com/sectors/telecommunication-services'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')\n",
    "\n",
    "# utilities\n",
    "lnk = 'https://www.estimize.com/sectors/utilities'\n",
    "get_est_file_stock_list(driver,proxy,proxy_pool,ind_link = lnk,op_file='stock_list_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Attempt to scrap LinkedIn\n",
    "\n",
    "* Includes attempt to mask the scrapping activity\n",
    "* Random time, clicks and hovering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, os, csv, platform\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "# from future import annotations\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pyautogui\n",
    "import random\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "log = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "\n",
    "# Disable webdriver flags or you will be easily detectable\n",
    "options.add_argument(\"--disable-blink-features\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading:  95%|█████████████████████████████████████████████████████▎  | 6.48M/6.81M [00:00<00:00, 29.4MB/s]<ipython-input-4-b480e122d08e>:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(ChromeDriverManager().install())\n",
      "[WDM] - Downloading: 100%|████████████████████████████████████████████████████████| 6.81M/6.81M [00:20<00:00, 29.4MB/s]"
     ]
    }
   ],
   "source": [
    "browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "browser.get(\"https://www.linkedin.com/login?trk=guest_homepage-basic_nav-header-signin\")\n",
    "\n",
    "# username = #fill here\n",
    "# password = #fill here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_field = browser.find_element(\"id\",\"username\")\n",
    "pw_field = browser.find_element(\"id\",\"password\")\n",
    "login_button = browser.find_element(\"xpath\",\n",
    "            '//*[@id=\"organic-div\"]/form/div[3]/button')\n",
    "user_field.send_keys(username)\n",
    "user_field.send_keys(Keys.TAB)\n",
    "time.sleep(2)\n",
    "pw_field.send_keys(password)\n",
    "time.sleep(2)\n",
    "login_button.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(\"https://www.linkedin.com/jobs/search/?currentJobId=3537172049&f_TPR=r86400&keywords=product%20management%20intern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait\n",
    "t = (random.randint(1,10))\n",
    "time.sleep(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get page\n",
    "page = browser.find_elements(\"xpath\",'//main[@id=\"main\"]')\n",
    "# browser.execute_script(\"window.stop();\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get elements on page\n",
    "max_search = 3000\n",
    "for i in range(101,max_search):\n",
    "    try:\n",
    "        txt = page[0].find_elements(\"xpath\",\n",
    "                                '//li[@id=\"ember{}\"]'.format(i))[0].text\n",
    "        res_list.append(txt)\n",
    "#         max_search=i+200\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# increase page number\n",
    "i+=1\n",
    "res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    next_pg = browser.find_element(\"xpath\",'//button[@aria-label=\"Page {}\"]'.format(9))\n",
    "    t = (random.randint(1,20))\n",
    "    time.sleep(t)\n",
    "except:\n",
    "    print(\"ded\")\n",
    "next_pg.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_():\n",
    "    # wait\n",
    "    t = (random.randint(1,5))\n",
    "    time.sleep(t)\n",
    "    \n",
    "def hover_(el,driver=browser):\n",
    "    action = ActionChains(driver)\n",
    "#     el = browser.find_elements(\"xpath\",'//main[@id=\"main\"]')[0]\n",
    "    action.move_to_element_with_offset(el, random.randint(1,20), random.randint(1,20))\n",
    "    action.click()\n",
    "    action.perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(\"https://www.linkedin.com/login?trk=guest_homepage-basic_nav-header-signin\")\n",
    "\n",
    "# username = 'fillhere@gmail.com'\n",
    "# password = \"*Pnp368#68456845\"\n",
    "wait_()\n",
    "user_field = browser.find_element(\"id\",\"username\")\n",
    "pw_field = browser.find_element(\"id\",\"password\")\n",
    "el = user_field\n",
    "wait_()\n",
    "\n",
    "login_button = browser.find_element(\"xpath\",\n",
    "            '//*[@id=\"organic-div\"]/form/div[3]/button')\n",
    "user_field.send_keys(username)\n",
    "user_field.send_keys(Keys.TAB)\n",
    "wait_()\n",
    "pw_field.send_keys(password)\n",
    "wait_()\n",
    "login_button.click()\n",
    "wait_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(\"https://www.linkedin.com/jobs/search/?currentJobId=3537172049&f_TPR=r86400&keywords=product%20management%20intern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "res = []\n",
    "itr=1\n",
    "\n",
    "\n",
    "for i in range(itr-1,100):\n",
    "    # refresh page\n",
    "    browser.refresh()\n",
    "    wait_()\n",
    "    \n",
    "    # increase iterator\n",
    "    itr+=1\n",
    "    \n",
    "    # get page\n",
    "    time.sleep(20)\n",
    "    page = browser.find_elements(\"xpath\",'//main[@id=\"main\"]')\n",
    "    wait_()\n",
    "#     browser.execute_script(\"window.stop();\")\n",
    "\n",
    "\n",
    "    for elm in page[0].text.split(\"Close\")[0].split(\"Job alert\")[1].replace(\"On-site\",\"||\").replace(\"Actively recruiting\",\"||\").replace(\"Promoted\",\"||\").replace(\"Within the past 24 hours\",\"||\").replace(\"ago\",\"||\").replace(\"Easy Apply\",\"||\").split(\"||\"):\n",
    "        if ((\"summer\" in elm.lower()) | (\"intern\" in elm.lower())):\n",
    "            res_list.append(elm)\n",
    "            for row in res_list:\n",
    "                res.append(row.split(\"\\n\"))\n",
    "\n",
    "    try:\n",
    "        next_pg = browser.find_element(\"xpath\",'//button[@aria-label=\"Page {}\"]'.format(itr))\n",
    "        wait_()\n",
    "    except:\n",
    "        print(\"ded\")\n",
    "        # refresh page\n",
    "#         browser.get(\"https://www.linkedin.com/jobs/search/?currentJobId=3537172049&f_TPR=r86400&keywords=product%20management%20intern\")\n",
    "        browser.refresh()\n",
    "        wait_()\n",
    "        itr-=1\n",
    "        \n",
    "    next_pg.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
